# ğŸš€ Scalable DataOps Pipeline with Airflow & AWS 

A modular data pipeline that simulates syncing MSP asset data, fully orchestrated with Apache Airflow and integrated with AWS S3.  
The project is designed to showcase production-grade skills relevant to a SaaS company.

---

## ğŸ§  Project Summary

This pipeline mimics a real-world SaaS use case for Managed Service Providers (MSPs). It automates the extraction of asset data, transforms it to meet quality standards, and uploads the cleaned result to Amazon S3 â€” ready for analytics or reporting.

---

## ğŸ› ï¸ Tech Stack

- **Apache Airflow 2.9** â€“ workflow orchestration
- **Docker Compose** â€“ containerized local environment
- **AWS S3** â€“ cloud storage for cleaned asset data
- **Python** â€“ ETL scripting and logging
- **GitHub Codespaces** â€“ cloud-based development
- *(Optional)* Redshift integration support

---

## ğŸ“¦ Folder Structure
```
Scalable-DataOps-Pipeline-with-Airflow-AWS/
â”œâ”€â”€ dags/                      # Airflow DAG definition
â”œâ”€â”€ scripts/                   # Reusable Python modules
â”œâ”€â”€ docker/                    # Docker Compose config
â”œâ”€â”€ .devcontainer/             # Codespaces setup
â”œâ”€â”€ .github/workflows/         # CI/CD (optional)
â”œâ”€â”€ .env                       # AWS + Redshift secrets (ignored by Git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ assets/                    # Diagrams or static images
```

---

## ğŸ” DAG Overview: `msp_asset_sync_pipeline`

| Task            | Description                                                  | Output File                          |
|-----------------|--------------------------------------------------------------|--------------------------------------|
| `extract_assets`| Simulates pulling asset info (e.g., backup status, devices)  | `/opt/airflow/tmp/assets.json`       |
| `transform_assets` | Cleans and standardizes fields                             | `/opt/airflow/tmp/transformed_assets.json` |
| `upload_to_s3`  | Uploads cleaned data to AWS S3                               | `s3://<your-bucket>/assets/...`      |

---

## ğŸ“‚ `.env` Configuration
```
# AWS Credentials & S3
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-west-2
AWS_S3_BUCKET=your-bucket-name

# Redshift (optional)
REDSHIFT_HOST=...
REDSHIFT_DB=...
REDSHIFT_USER=...
REDSHIFT_PASSWORD=...
REDSHIFT_PORT=5439
```

## ğŸ§ª How to Run Locally
	1.	Clone the repository:
    ```
    git clone https://github.com/YOUR_USERNAME/Scalable-DataOps-Pipeline-with-Airflow-AWS.git
    cd Scalable-DataOps-Pipeline-with-Airflow-AWS
    ```
    2.	Configure .env
	â€¢	Copy the example and add your AWS keys and bucket.

	3.	Launch with Docker
    ```
    cd docker
    docker-compose up airflow-init
    docker-compose up
    ```
    	4.	Access Airflow UI
	â€¢	Go to: http://localhost:8080
	â€¢	Trigger the DAG: msp_asset_sync_pipeline
    
##  ğŸ“Š Architecture Diagram

Include diagram here once created (e.g., ETL â†’ S3 â†’ Redshift)

â¸»

ğŸ’¡ Future Enhancements
	â€¢	Redshift loader for BI/analytics
	â€¢	Slack/Email alerts on failure
	â€¢	Unit tests & CI integration
	â€¢	API-based real data ingestion   

ğŸ‘©â€ğŸ’» Author

Bita Ashoori
LinkedIn | GitHub